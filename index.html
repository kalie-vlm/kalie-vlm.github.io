<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data">
  <meta name="keywords" content="Vision-Language Model, Data Synthesis, Fine-Tuning, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>KALIE</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <!-- <div class="container is-fullhd"> -->
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data</h1>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block">
            <a href="https://kuanfang.github.io">Kuan Fang*</a></span>,
          <span class="author-block">
            <a href="https://fangchenliu.github.io">Fangchen Liu*</a></span>,
          <span class="author-block">
            <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
          </span>
          <span class="author-block">
            <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
          </span>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>*</sup> denotes equal contribution, alphabetical order</span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block">Berkeley AI Research, UC Berkeley</span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="my-block">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Building generalist robotic systems involves effectively endowing robots the capabilities to handle novel objects in an open-world setting. Inspired by the advances of large pre-trained models, we propose Keypoint Affordance Learning from Imagined Environments (KALIE), which adapts pre-trained Vision Language Models (VLMs) for robotic control in a scalable manner. Instead of directly producing motor commands, KALIE controls the robot by predicting point-based affordance representations based on natural language instructions and visual observations of the scene. The VLM is trained on 2D images with affordances labeled by humans, bypassing the need for training data collected on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE automatically creates massive high-quality training data based on limited example data manually collected by humans. We demonstrate that KALIE can learn to robustly solve new manipulation tasks with unseen objects given only 50 example data points. Compared to baselines using pre-trained VLMs, our approach consistently achieves superior performance.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>

  <div class="container is-fullhd">
    <div class="my-block">
        <div class="column is-full-width">
          <h2 class="title is-3">Keypoint Affordance Learning from Imagined Environments (KALIE)</h2>
          
          <h3 class="content has-text-justified">
            By fine-tuning a pre-trained VLM, KALIE predicts the <b>point-based affordance representation</b> given the observed image, the task instruction, and a text prompt. The point-based affordance specifies the desired motion to complete the task.

            <br/>
            <br/>

            Starting with limited example data collected from the real world, KALIE generates synthetic data with high <b>diversity</b> and <b>quality</b>, while preserving the <b>task semantics</b> and <b>keypoint annotations</b>. The fine-tuned VLM can robustly generate motions unseen objects and scene arrangements.

            <!-- <br/> -->
            <!-- <br/> -->
            <!--  -->
            <!-- The VLM predicts a point-based affordance representation given  the observed image, the task instruction, and a text prompt. The point-based affordance specifies the desired motion to complete the task. -->
          </h3>

          <div>
            <img src="images/model_pipeline.png"
                       class="interpolation-image"
                       alt="Interpolation end reference image."/>
          </div>

          <div style="height: 10px;"></div>
          <br/>
          <br/>

          <h3 class="title is-3">Affordance-Aware Data Synthesis</h3>

          <h3 class="content has-text-justified">
            KALIE employs the inpainting capability of a pre-trained diffusion model to generate synthetic data. 
            <!-- <br/> -->
            <!-- <br/> -->
            To diversify the scenes while staying faithful to the task semantics and the keypoint annotations, KALIE uses a <b>context image</b> as additional inputs to the diffusion model, which specifies the geometric properties of the object to be inpainted. The context image is computed from the original RGB image and can be randomly transformed to further diversify the synthetic data.

          </h3>

          <div>
            <img src="images/diffusion_pipeline.png"
                       class="interpolation-image"
                       alt="Interpolation end reference image."/>
          </div>

      </div>
    </div>

    <div class="my-block">
      <div class="column is-full-width">
        <h2 class="title is-3">Synthetic Data</h2>

          <h3 class="content has-text-justified">
            In each column, we show example synthetic images generated based on two example images respectively for the same task. The original and transformed point-based affordances are plotted on top of the images.
          </h3>

        <div>
        <div class="columns is-centered has-text-centered">
          <img src="images/imagination_examples_supplementary.png"
                     class="interpolation-image"
                     alt="Interpolation end reference image."
                     style="width: 80%;"
                     />
            </div>
          </div>
      </div>
    </div>

    <div class="my-block">

      <div class="rows is-fullhd">
        <h2 class="title is-3">Task Execution</h2>
        
        <p class="content has-text-justified">
          KALIE can solve manipulation tasks involving diverse unseen objects and initial arrangements. 
        </p>

        <div class="rows">
          <p style="text-align:center"><strong>Sweep the trash off the table</strong></p>
          <div style="height: 10px;"></div>
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/sweeping_vid1_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/sweeping_vid2_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/sweeping_vid3_text.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        
        <div style="height: 20px;"></div>

        <div class="rows">
          <p style="text-align:center"><strong>Close the drawer</strong></p>
          <div style="height: 10px;"></div>
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/drawer_vid1_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/drawer_vid2_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/drawer_vid3_text.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div style="height: 20px;"></div>
        
        <div class="rows">
          <p style="text-align:center"><strong>Hang the towel on the rack</strong></p>
          <div style="height: 10px;"></div>
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/towel_vid1_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/towel_vid2_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/towel_vid3_text.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div style="height: 20px;"></div>
        
        <div class="rows">
          <p style="text-align:center"><strong>Pour small objects into the bowl</strong></p>
          <div style="height: 10px;"></div>
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/pouring_vid1_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/pouring_vid2_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist1" controls muted autoplay loop width="99%">
                <source src="videos/pouring_vid3_text.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div style="height: 20px;"></div>
        
        <div class="rows">
          <p style="text-align:center"><strong>Unplug the USB stick from the laptop</strong></p>
          <div style="height: 10px;"></div>
          <div class="columns">
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/usb_vid1_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/usb_vid2_text.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column has-text-centered">
              <video id="dist2" controls muted autoplay loop width="99%">
                <source src="videos/usb_vid3_text.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        
      </div>
    </div>

		</br>
		</br>

     <div class="my-block">
      <div class="column is-full-width">
        <h2 class="title is-3">Acknowledgement</h2>
        <div>
          <h2 class="content has-text-justified">
            The robot infra in our experiments follow the <a href="https://droid-dataset.github.io/droid/hardware-setup/shopping-list.html">DROID</a> setup using a Franka Research 3 (FR3) robot.
          </h2>
        </div>
      </div>
    </div>

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from nerfie's <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
